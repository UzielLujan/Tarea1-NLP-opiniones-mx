# üß† Contexto: Tarea 1 - NLP (Maestr√≠a en C√≥mputo Cient√≠fico)

Este documento describe el pipeline actualizado y las decisiones de dise√±o para la **Tarea 1 del curso An√°lisis de Texto e Im√°genes con Deep Learning**.

---

## üì¶ Estructura modular y flujo de procesamiento

- **Preprocesamiento flexible:**  
  - `src/preprocessing/limpieza.py` ahora separa la lectura b√°sica (`leer_corpus`) del procesamiento avanzado (`procesar_corpus`).
  - El corpus se lee una sola vez y se procesa solo con el nivel de limpieza requerido por cada m√≥dulo.
  - Los pasos de limpieza avanzados (normalizaci√≥n, stopwords, lematizaci√≥n/stemming) se controlan por par√°metros.

- **M√≥dulos independientes y eficientes:**  
  - Cada m√≥dulo (`analysis/`, `representaciones/`, `embeddings/`, `clustering/`, `modeling/`, `topics/`) recibe el corpus ya procesado seg√∫n sus necesidades.
  - No hay doble procesamiento ni redundancias.
  - El pipeline es f√°cil de mantener y permite experimentar con variantes de limpieza sin modificar los m√≥dulos internos.

- **main.py como orquestador:**  
  - Usa `argparse` para ejecutar cada bloque por separado o el pipeline completo.
  - Solo procesa el corpus con el nivel de limpieza necesario para cada experimento.
  - Ejemplo de flujo:
    - Estad√≠sticas: corpus solo con limpieza b√°sica.
    - Zipf: corpus con o sin stopwords seg√∫n argumento.
    - Palabras frecuentes/POS: corpus normalizado y sin stopwords.
    - BoW/TF-IDF/Word2Vec: corpus normalizado y sin stopwords.
    - Clasificaci√≥n: variantes acumulativas de procesamiento (sin preprocesamiento, min√∫sculas, lematizaci√≥n, etc.).
    - LSA: usa la matriz TF-IDF generada.

- **Persistencia y salidas organizadas:**  
  - `data/` y `reports/` organizados para guardar salidas (.csv, .png, .npy, etc.).
  - Vectorizadores, embeddings y resultados de clustering/modelado se guardan para an√°lisis posterior.

---

## üß™ Entorno y dependencias

- **Librer√≠as clave:**  
  - `nltk`, `spacy`, `scikit-learn`, `gensim`, `matplotlib`, `seaborn`, `wordcloud`
- **Modelo spaCy:**  
  - `es_core_news_sm` instalado y cargado solo una vez.
- **Compatibilidad:**  
  - Python 3.10, conflictos de dependencias resueltos.
  - OpenMP (`libiomp5md.dll`) y conflictos `dtype` entre `numpy` y `scikit-learn` documentados y resueltos.

---

## üõ†Ô∏è Ejecuci√≥n y ejemplos de uso

```bash
python main.py --zipf --sin_stopwords --top_n 100
python main.py --frecuentes
python main.py --bow --ngram_max 2 --min_df 10
python main.py --word2vec --cluster
python main.py --clasificacion
python main.py --lsa
```

- Puedes combinar flags para ejecutar varios m√≥dulos en una sola corrida.
- El pipeline completo se ejecuta con `--pipeline_completo`.

---

## üé® Visualizaci√≥n

- Estilo consistente con el notebook de EDA:
  - Paleta `"Reds"`
  - Barras con etiquetas de conteo
  - Nubes de palabras con `WordCloud`
  - Gr√°ficas por clase (`Polarity`, `Type`, `Town`)

---

## üîç Decisiones de dise√±o y mejores pr√°cticas

- **Separaci√≥n de lectura y procesamiento:**  
  Permite m√°xima flexibilidad y eficiencia, evitando recargas y reprocesamientos innecesarios.
- **Procesamiento parametrizable:**  
  Los pasos de limpieza se aplican solo cuando y como se necesitan.
- **M√≥dulos desacoplados:**  
  Cada m√≥dulo asume que el corpus ya est√° en el estado requerido.
- **Facilidad para experimentaci√≥n:**  
  Cambiar el nivel de limpieza para cualquier experimento es trivial y no requiere modificar los m√≥dulos internos.
- **Soporte para an√°lisis avanzados:**  
  Filtrado de palabras custom en an√°lisis de palabras frecuentes, experimentos acumulativos en clasificaci√≥n, etc.

---

## üß™ Pendientes y mejoras sugeridas

- Crear `tests/` con `pytest` para validaci√≥n modular.
- Agregar visualizaciones adicionales (`plot_top_n_categorica`, etc.).
- Integrar todo en reporte LaTeX.
- Mejorar documentaci√≥n de cada m√≥dulo y agregar ejemplos de uso.

---

Este documento sirve como referencia t√©cnica para continuar, compartir o migrar el proyecto sin perder el contexto y las decisiones clave del pipeline.